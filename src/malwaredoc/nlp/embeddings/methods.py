from typing import List, Optional

from dotenv import load_dotenv

load_dotenv()
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

from malwaredoc.nlp.embeddings.models import MULTILINGUAL_EMBEDDING_MODEL
from malwaredoc.nlp.embeddings.util import _weighted_average


def embed_text(text: str, model: HuggingFaceEmbeddings) -> Optional[List[float]]:
    return model.embed_query(text) if text else None


def summary_embeddings(
    text: str,
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    model: HuggingFaceEmbeddings = MULTILINGUAL_EMBEDDING_MODEL,
) -> Optional[list[float]]:
    """
    Calculate embeddings by splitting text and averaging over all sections.
    Args:
        model: model to use for embedding
        chunk_overlap: chunk overlap.
        chunk_size: chunk size.
        text: text to embed.

    Returns:
        embedding vector.
    """
    if text:
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len, keep_separator=True
        )
        sections = text_splitter.split_text(text)
        embeddings = [model.embed_query(section) for section in sections]
        return _weighted_average(embeddings, [len(section) for section in sections])
    return None
