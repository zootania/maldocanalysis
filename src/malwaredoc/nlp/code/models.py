"""LLM models for document analysis."""
from typing import Any, List, Mapping, Optional

import torch
from dotenv import load_dotenv

load_dotenv()
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM
from transformers import (
    AutoModel,
    AutoTokenizer,
    T5ForConditionalGeneration,
    T5Tokenizer,
)


class ChatGLM(LLM):
    """Large LLM for text analysis.

    See https://huggingface.co/THUDM/chatglm-6b for further details.
    """

    _model_name = "THUDM/chatglm-6b"
    _code_revision = "619e736c6d4cd139840579c5482063b75bed5666"
    _tokenizer = None
    _model = None

    @classmethod
    def _load_model(cls):
        if not cls._model:
            cls._model = (
                AutoModel.from_pretrained(cls._model_name, trust_remote_code=True, revision=cls._code_revision)
                .half()
                .cuda()
            )
        if not cls._tokenizer:
            cls._tokenizer = AutoTokenizer.from_pretrained(
                cls._model_name,
                trust_remote_code=True,
                revision=cls._code_revision,
                max_length=4096,
            )

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        if not self._model:
            self._load_model()
        response, _ = self._model.chat(self._tokenizer, prompt, history=[])
        return response

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"name_of_model": "chatglm-6b"}

    @property
    def _llm_type(self) -> str:
        return "ChatGLM"


class ChatGLMInt4(ChatGLM):
    """Int 4 Quantized version of ChatGLM."""

    _model_name = "THUDM/chatglm-6b-int4"
    _code_revision = "6c5205c47d0d2f7ea2e44715d279e537cae0911f"

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"name_of_model": "chatglm-6b-int4"}

    @property
    def _llm_type(self) -> str:
        return "ChatGLMInt4"


class FlanT5Base(LLM):
    """Small model that can be used for sentiment analysis.

    This is still a text generation model, not a classification model,
    thus needs to be used with a relevant prompt.
    See https://huggingface.co/google/flan-t5-base for further details.
    """

    _model_name = "google/flan-t5-base"
    _code_revision = "c782cba52f8ea6a704240578055cf1c3fc2f2ca9"
    _tokenizer = None
    _model = None

    @classmethod
    def _load_model(cls):
        if not cls._tokenizer:
            cls._tokenizer = T5Tokenizer.from_pretrained(cls._model_name, revision=cls._code_revision)
        if not cls._model:
            cls._model = T5ForConditionalGeneration.from_pretrained(
                cls._model_name,
                revision=cls._code_revision,
                device_map="auto",
                torch_dtype=torch.float16,
            )

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        if not self._model:
            self._load_model()
        input_ids = self._tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")
        outputs = self._model.generate(input_ids, max_length=200)
        return self._tokenizer.decode(outputs[0], skip_special_tokens=True)

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"name_of_model": "flan-t5-base"}

    @property
    def _llm_type(self) -> str:
        return "FlanT5Base"


class FlanT5Large(FlanT5Base):
    _model_name = "google/flan-t5-large"
    _code_revision = "0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a"
    _tokenizer = None
    _model = None

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"name_of_model": "flan-t5-large"}

    @property
    def _llm_type(self) -> str:
        return "FlanT5Large"
