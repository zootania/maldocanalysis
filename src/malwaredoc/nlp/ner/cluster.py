import itertools
import json
import warnings
from pathlib import Path

import pandas as pd
import ray
from dotenv import load_dotenv
from langchain.embeddings import HuggingFaceEmbeddings
from ray import tune
from ray.tune.search.optuna import OptunaSearch
from sklearn.cluster import HDBSCAN
from sklearn.metrics.pairwise import cosine_similarity

from malwaredoc.config import load_config
from malwaredoc.nlp.ner.methods import EntityType, NamedEntity, _select_entities
from malwaredoc.util.filesystem import (
    create_dir,
    load_dict_from_file,
    save_dict_to_file,
)

warnings.simplefilter(action="ignore", category=pd.errors.PerformanceWarning)
_CONFIG_PATH = Path("../../../../data/clustering/feature-config.toml").resolve().absolute()
_FEATURES_PATH = Path("../../../../data/clustering/features.parquet").resolve().absolute()
_RESULT_PATH = Path("../../../../data/clustering/ner/runs/").resolve().absolute()
_ENTITY_PATH = Path("../../../../data/clustering/ner/entities/entities.pickle").resolve().absolute()
_GROUND_TRUTH_PATH = Path("../../../../data/clustering/ground_truth.json")
_CLUSTERING_PATH = Path("../../../../data/clustering/").resolve().absolute()
load_dotenv()


def _calculate_score(ground_truth: list[dict], df: pd.DataFrame) -> int:
    points = 0
    for gt in ground_truth:
        files = gt["files"]
        groups = df.loc[files, "group"]
        if len(groups.unique()) == 1:
            points += 1
    return points


def _combine_lists(df: pd.DataFrame) -> dict:
    result = {}
    for col in df.columns:
        result[col] = []
        for row in df[col]:
            result[col].extend(row)
    return result


def _get_unique_entities(entities: list[NamedEntity]) -> list[NamedEntity]:
    """
    Get unique list of entities based on entity word.
    Args:
        entities: entities.

    Returns:
        list of unique entities in list.
    """
    str_entities = {e.word for e in entities}
    unique_entities = []
    for entity in entities:
        if entity.word in str_entities:
            unique_entities.append(entity)
            str_entities.remove(entity.word)
    return unique_entities


def cluster_ner(config: dict, entities=None, features=None, first_submitted=None, ground_truth=None, ner_model=None):
    ner_acceptance_threshold: float = config["ner_acceptance_threshold"]
    ner_clustering_threshold: float = config["ner_clustering_threshold"]
    min_cluster_size: int = config["min_cluster_size"]
    permitted_entities: list[EntityType] = config["permitted_entities"]
    first_submitted_interval: str = config["first_submitted_interval"]
    min_word_length: int = config["min_word_length"]
    dir_name_dict = {
        "ner_acceptance_threshold": ner_acceptance_threshold,
        "ner_clustering_threshold": ner_clustering_threshold,
        "min_cluster_size": min_cluster_size,
        "permitted_entities": [v.value for v in permitted_entities],
        "first_submitted_bin_interval": first_submitted_interval,
    }
    dir_name_string = json.dumps(dir_name_dict)
    dir_name_hash = hash(dir_name_string)
    dir_name = _RESULT_PATH / f"run_{dir_name_hash}/"

    entities_list = list(itertools.chain.from_iterable([value for key, value in _combine_lists(entities).items()]))
    relevant_entities = _select_entities(
        entities=entities_list,
        permitted_entities=permitted_entities,
        threshold=ner_acceptance_threshold,
        min_word_length=min_word_length,
    )
    unique_entities = _get_unique_entities(relevant_entities)
    entity_embeddings = pd.DataFrame()
    entity_embeddings["entity"] = [e.word for e in unique_entities]
    entity_embeddings["embedding"] = [ner_model.embed_query(e.word) for e in unique_entities]
    similarity_matrix = cosine_similarity(entity_embeddings["embedding"].tolist())
    mask = similarity_matrix >= ner_clustering_threshold
    groups = []
    for i, row in enumerate(mask):
        if row[i]:
            group = entity_embeddings.iloc[row]["entity"].tolist()
            groups.append(group)
    entity_embeddings["similar"] = groups
    encoded_entities = pd.DataFrame(index=entities.index)
    encoded_entities["entities"] = entities.apply(
        lambda row: list({item.word for cell in row for item in cell}), axis=1
    )
    feature_names = entity_embeddings["entity"].tolist()
    encoded_entities = pd.concat([encoded_entities, pd.DataFrame(columns=feature_names)], axis=1)
    encoded_entities.fillna(0, inplace=True)
    for i, row in entity_embeddings.iterrows():
        entity = row["entity"]
        for value in row["similar"]:
            if value in feature_names:
                encoded_entities.loc[encoded_entities["entities"].apply(lambda x: entity in x), value] = 1
    encoded_entities["source_file_name"] = [file_name.split(".")[0] for file_name in encoded_entities.index.tolist()]
    dates = pd.to_datetime(list(first_submitted.values()), unit="s")
    bins = pd.date_range(start=dates.min(), end=dates.max(), freq=first_submitted_interval)
    binned_dates = pd.cut(dates, bins)
    one_hot = pd.get_dummies(binned_dates, dtype="int")
    one_hot["source_file_name"] = first_submitted.keys()
    encoded_entities = pd.merge(encoded_entities, one_hot, on="source_file_name", how="inner")
    encoded_entities.index = features.index
    encoded_entities.drop(["entities", "source_file_name"], axis=1, inplace=True)
    encoded_entities.columns = [str(column) for column in encoded_entities.columns]
    hdb = HDBSCAN(min_cluster_size=min_cluster_size)
    hdb.fit(encoded_entities)
    encoded_entities["group"] = hdb.labels_
    score = _calculate_score(ground_truth, df=encoded_entities)
    metadata = {
        "ner_acceptance_threshold": ner_acceptance_threshold,
        "ner_clustering_threshold": ner_clustering_threshold,
        "min_cluster_size": min_cluster_size,
        "permitted_entities": [v.value for v in permitted_entities],
        "first_submitted_bin_interval": first_submitted_interval,
        "score": score,
    }
    create_dir(dir_name)
    save_dict_to_file(metadata, dir_name / "metadata.json")
    encoded_entities.to_parquet(dir_name / "encoded_entities.parquet")
    return {"score": score, "dir_name": dir_name}


if __name__ == "__main__":
    search_space = {
        "ner_acceptance_threshold": tune.uniform(lower=0.6, upper=1),
        "ner_clustering_threshold": tune.uniform(lower=0.6, upper=1),
        "min_cluster_size": tune.choice([2, 5, 10, 20]),
        "first_submitted_interval": tune.choice(["1M", "3M", "6M", "12M"]),
        "min_word_length": tune.choice([2, 3]),
        "permitted_entities": tune.choice(
            [
                [EntityType.PER],
                [EntityType.PER, EntityType.ORG],
                [EntityType.PER, EntityType.LOC],
                [EntityType.ORG, EntityType.PER, EntityType.LOC],
                list(EntityType),
            ]
        ),
    }

    runtime_env = {
        "env_vars": {
            "HF_HOME": "G:\Virtualbox Images\shared\malwaredoc\cache\hf\misc",
            "HF_DATASETS_CACHE": "G:\Virtualbox Images\shared\malwaredoc\cache\hf\datasets",
            "TRANSFORMERS_CACHE": "G:\Virtualbox Images\shared\malwaredoc\cache\hf\models",
        }
    }

    # start ray runtime
    ray.init(runtime_env=runtime_env)

    # load relevant large files from disk
    config = load_config(_CONFIG_PATH)
    first_submitted = load_dict_from_file(Path("../../data/clustering/first_submitted.json"))
    ground_truth = load_dict_from_file(Path("../../data/clustering/ground_truth.json"))
    entities = pd.read_pickle(_ENTITY_PATH)
    model = HuggingFaceEmbeddings(model_name="bert-base-multilingual-cased")
    features = pd.read_parquet(_FEATURES_PATH)
    trainable_with_params = tune.with_parameters(
        cluster_ner,
        ner_model=model,
        entities=entities,
        features=features,
        first_submitted=first_submitted,
        ground_truth=ground_truth,
    )
    trainable_with_resources = tune.with_resources(trainable_with_params, {"cpu": 4})
    tuner = tune.Tuner(
        trainable_with_resources,
        tune_config=tune.TuneConfig(search_alg=OptunaSearch(), num_samples=100, metric="score", mode="max"),
        param_space=search_space,
    )

    results = tuner.fit()

    results.get_dataframe().to_parquet(_RESULT_PATH / "results.parquet")

    ray.shutdown()
