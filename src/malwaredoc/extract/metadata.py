"""Tools for extracting metadata."""

import json
import logging
import os
import re
import subprocess
import zipfile
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, get_type_hints

import exiftool

import filetype
from pydantic import ConfigDict, ValidationError, ValidationInfo, field_validator
import rarfile
import win32com.client
from pypdf import PdfReader
from pypdf.xmp import XmpInformation
from pydantic import validator
from magika import Magika

from malwaredoc.data.base import Base, Dictable, ValidatorModel
from malwaredoc.data.types import FileType, HexBytes
from malwaredoc.extract.util import convert_string_to_attribute_name
from malwaredoc.config import WIN_7Z_PATH

class MagikaMeta(ValidatorModel, Base, Dictable):
    model_config = ConfigDict(extra='ignore')
    ct_label: Optional[str]
    score: Optional[float]
    group: Optional[str]
    mime_type: Optional[str]
    magic: Optional[str]
    description: Optional[str]
    path: Optional[str]

    def to_dict(self) -> Dict[str, Any]:
        return self.model_dump()
    
class ExifMeta(ValidatorModel,Base, Dictable):
    model_config = ConfigDict(extra='ignore')  
    
    sourcefile: Optional[str]
    exiftoolversion: Optional[float]
    filename: Optional[str]
    directory: Optional[str]
    filesize: Optional[str]
    filemodifydate: Optional[str]
    fileaccessdate: Optional[str]
    filecreatedate: Optional[str]
    filepermissions: Optional[str]
    filetype: Optional[str]
    filetypeextension: Optional[str]
    mimetype: Optional[str]
    ziprequiredversion: Optional[int]
    zipbitflag: Optional[str]
    zipcompression: Optional[str]
    zipmodifydate: Optional[str]
    zipcrc: Optional[str]
    zipcompressedsize: Optional[int]
    zipuncompressedsize: Optional[int]
    zipfilename: Optional[str]
    creator: Optional[str]
    lastmodifiedby: Optional[str]
    createdate: Optional[str]
    modifydate: Optional[str]
    application: Optional[str]
    docsecurity: Optional[str]
    scalecrop: Optional[str]
    headingpairs: Optional[list[str]]
    titlesofparts: Optional[list[str]]
    company: Optional[str]
    linksuptodate: Optional[str]
    shareddoc: Optional[str]
    hyperlinkschanged: Optional[str]
    appversion: Optional[float]
    contenttypeid: Optional[str]
    totaledittime: Optional[str]

    def to_dict(self) -> Dict[str, Any]:
        return self.model_dump()

def extract_exif_features(file: Path) -> Optional[ExifMeta]:
    """
    Extracts EXIF metadata from a given file using ExifTool.

    Args:
        file: The path to the file from which to extract EXIF data.

    Returns:
        A dictionary with EXIF metadata, following the structure defined by ExifDataDict.
             Returns an empty dictionary if no EXIF data is found or an error occurs.
    """
    exif_info: ExifMeta = None
    exif_tool_results: Optional[dict[str, Any]] = None
    try:
        exif_tool_results = ExifInfo().get_exiftool_json(file)
    except Exception as exif_parsing_error:
        logging.error(f"exif parsing failed on file: {exif_parsing_error}")
        return exif_info
    if not exif_tool_results:
        return exif_info

    # Convert keys to attribute names and fill missing keys with None
    remap = {convert_string_to_attribute_name(k): v for k, v in exif_tool_results.items()}
    remap = {key: remap.get(key, None) for key in ExifMeta.model_fields.keys()}

    try:
        return ExifMeta.model_validate(remap, strict=False)
    except ValidationError as exc:
        print(f"Validation error while processing EXIF data for {file}: {exc}")
        return exif_info

def extract_magika_features(file_path: Path) -> dict[str, Union[str, float]]:
    """ Returns the magika file information as a dict.

    Args:
        file_path: The file path that we want to scan
    Returns:
        The magika properties as a dictionary. Note we only return the output and not deep learning results.
        This is because the output filters the deep learning results to improve fidelity.
    """
    magika_res = Magika().identify_path(file_path)
    # If magika results are none
    if not magika_res:
        return {}
    type_to_use = magika_res.output
    # If the output type is unknown we use the deep learning guess results.
    if type_to_use.ct_label == "unknown":
        type_to_use = magika_res.dl
    magika_props: dict[str, Union[str, float]] =  {
        "ct_label": type_to_use.ct_label,
        "group": type_to_use.group,
        "magic": type_to_use.magic,
        "description": type_to_use.description,
        "mime_type": type_to_use.mime_type,
        "score": type_to_use.score,
        "path": magika_res.path
    }
    try:
        return MagikaMeta.model_validate(magika_props, strict=False)
    except ValidationError as exc:
        print(f"Validation error while processing Magika data for {file_path}: {exc}")
        return {}


class FileTypeData(Base, Dictable):
    type_magic: Optional[str]
    mime_magic: Optional[str]
    type_filetype: Optional[FileType]
    mime_filetype: Optional[str]
    extension_filetype: Optional[str]
    additional_info: Optional[Dict[str, str]]

    def to_dict(self) -> Dict[str, Any]:
        return self.model_dump()  # values in additional info might not be relevant


def _get_additional_information_from_file_type(type_info: str) -> Dict[str, str]:
    """Parse information in a type information string.

    Args:
        type_info (str): magic-produced type string.

    Returns:
        Dict[str, str]: Attribute - Info mapping
    """
    pieces = re.split(r",(?=[^,:]*:)", type_info)
    infos = {}
    for piece in pieces:
        items = piece.split(":")
        if len(items) == 2:
            infos[items[0].strip()] = items[1].strip()
    return infos

class ExifInfo():
    project_directory = os.path.dirname(__file__)
    def __init__(self, exif_tool_path: str = os.path.join(project_directory, r"bin/exiftool.exe")) -> None:
        """ Generates the exif info using the the existing exif tool path
    
        Args:
             exif_tool_path: The location of the exif_tool_binary
        """

        self.exif_tool_path = exif_tool_path

    def get_exiftool_json(self, file_path: str, parsing_charset:str ="latin1") -> Optional[Union[dict[str, Any], None]]:
        """
        Get Exiftool output in JSON format for a given file with a specific charset (Latin-1).

        Parameters:
            file_path (str): The path to the file.
            parsing_charset: What encoding to process the file with

        Returns:
            dict or None: A dictionary containing the Exiftool information in JSON format,
                        or None if an error occurs.
        """
        try:
            exiftool_command = [self.exif_tool_path, "-j", "-charset", parsing_charset, file_path]
            exiftool_output = subprocess.check_output(exiftool_command, universal_newlines=True, encoding=parsing_charset)
            exif_data = json.loads(exiftool_output)[0]
            return exif_data
        except subprocess.CalledProcessError as e:
            logging.debug(f"Error handling output from subprocess: {e}")
            return None
        except Exception as all_other_exceptions:
            logging.debug(f"Error running Exiftool: {all_other_exceptions}")
            return None

def get_file_type(file: Path) -> FileTypeData:
    """Determine the type of a file.

    Args:
        file (Path): The path to the file.

    Returns:
        FileTypeData: Object containing file type information.
    """
 
    exif_info = ExifInfo()
    try:
        exif_res = exif_info.get_exiftool_json(file)
    except Exception as exif_error:
        logging.error(f"Could not get file type from exif meta: {file} reason {exif_error}")
    file_type, mime_magic = "", ""
    if exif_res:
        file_type = exif_res.get("FileType", "")
        mime_magic = exif_res.get("MIMEType", "")
    else:
        magic_meta = extract_magika_features(file)
        mime_magic = magic_meta.mime_type
        file_type = magic_meta.ct_label
    return FileTypeData(
        type_magic=file_type,
        mime_magic=mime_magic,
        type_filetype=filetype.guess(file),
        mime_filetype=filetype.guess_mime(file),
        extension_filetype=filetype.guess_extension(file),
        additional_info=_get_additional_information_from_file_type(file_type),
    )



class XMPMetaData(Base, Dictable):
    """See https://pypdf.readthedocs.io/en.stable/modules/XmpInformation.html for more information."""

    contributor: Optional[List[str]]
    coverage: Optional[str]
    creator: Optional[List[str]]
    date: Optional[List[str]]
    description: Optional[Dict[str, str]]
    format: Optional[str]
    identifier: Optional[str]
    language: Optional[List[str]]
    publisher: Optional[List[str]]
    relation: Optional[List[str]]
    rights: Optional[Dict[str, str]]
    source: Optional[str]
    subject: Optional[str]
    title: Optional[Dict[str, str]]
    type: Optional[List[str]]
    keywords: Optional[str]
    version: Optional[str]
    producer: Optional[str]
    create_date: Optional[str]
    modify_date: Optional[str]
    metadata_date: Optional[str]
    creator_tool: Optional[str]
    document_id: Optional[str]
    instance_id: Optional[str]
    properties: Optional[Dict[str, str]]

    def to_dict(self) -> Dict[str, Any]:
        return {
            "contributors": ",".join(self.contributor) if self.contributor else "",
            "coverage": self.coverage or "",
            "creators": ",".join(self.creator) if self.creator else "",
            "format": self.format or "",
            "identifier": self.identifier or "",
            "languages": ",".join(self.language) if self.language else "",
            "publishers": ",".join(self.publisher) if self.publisher else "",
            "relations": ",".join(self.relation) if self.relation else "",
            "types": ",".join(self.type) if self.type else "",
            "source": self.source or "",
            "subject": self.subject or "",
            "version": self.version or "",
            "keywords": self.keywords or "",
            "producer": self.producer or "",
            "create_date": self.create_date or "",
            "modify_date": self.modify_date or "",
            "metadata_date": self.metadata_date or "",
            "creator_tool": self.creator_tool or "",
            "document_id": self.document_id or "",
            "instance_id": self.instance_id or "",
        }


def _parse_xmp_metadata(xmp_meta: XmpInformation) -> Optional[XMPMetaData]:
    """Parse extraced xmp metadata from a pdf.

    Args:
        xmp_meta (XmpInformation): xmp metadata.

    Returns:
        XMPMetaData: xmp metadata.
    """
    if xmp_meta:
        return XMPMetaData(
            contributor=xmp_meta.dc_contributor,
            coverage=xmp_meta.dc_coverage,
            creator=xmp_meta.dc_creator,
            date=xmp_meta.dc_date,
            description=xmp_meta.dc_description,
            format=xmp_meta.dc_format,
            identifier=xmp_meta.dc_identifier,
            language=xmp_meta.dc_language,
            publisher=xmp_meta.dc_publisher,
            relation=xmp_meta.dc_relation,
            rights=xmp_meta.dc_rights,
            source=xmp_meta.dc_source,
            subject=xmp_meta.dc_subject,
            title=xmp_meta.dc_title,
            type=xmp_meta.dc_type,
            keywords=xmp_meta.pdf_keywords,
            version=xmp_meta.pdf_pdfversion,
            producer=xmp_meta.pdf_producer,
            create_date=xmp_meta.xmp_create_date,
            modify_date=xmp_meta.xmp_modify_date,
            metadata_date=xmp_meta.xmp_metadata_date,
            creator_tool=xmp_meta.xmp_creator_tool,
            document_id=xmp_meta.xmpmm_document_id,
            instance_id=xmp_meta.xmpmm_instance_id,
            properties=xmp_meta.custom_properties,
        )
    else:
        return None


class PDFMetadata(Base, Dictable):
    author: Optional[str]
    creation_date: Optional[str]
    creator: Optional[str]
    modification_date: Optional[str]
    producer: Optional[str]
    subject: Optional[str]
    title: Optional[str]
    xmp_metadata: Optional[XMPMetaData]
    is_encrypted: bool

    def to_dict(self) -> Dict[str, Any]:
        return {
            "author": self.author or "",
            "creation_date": self.creation_date or "",
            "creator": self.creator or "",
            "modification_date": self.modification_date or "",
            "producer": self.producer or "",
            "subject": self.subject or "",
            "title": self.title or "",
            "xmp_metadata": self.xmp_metadata.to_dict() if self.xmp_metadata else None,
            "is_encrypted": self.is_encrypted,
        }


def get_pdf_metadata(file: Path) -> Optional[PDFMetadata]:
    """Extract metadata from pdf.

    Args:
        file (Path): path to pdf.

    Returns:
        Optional[PDFMetadata]: meta data of pdf.
    """
    try:
        reader = PdfReader(file.resolve())
    except Exception:
        print(f"Failed to read pdf file: {file}")
        return None
    if reader.is_encrypted or reader.metadata is None:
        return PDFMetadata(
            author=None,
            creation_date=None,
            creator=None,
            modification_date=None,
            producer=None,
            subject=None,
            title=None,
            xmp_metadata=None,
            is_encrypted=reader.is_encrypted,
        )
    meta = reader.metadata
    return PDFMetadata(
        author=meta.author,
        creation_date=meta.creation_date_raw,
        creator=meta.creator,
        modification_date=meta.modification_date_raw,
        producer=meta.producer,
        subject=meta.subject,
        title=meta.title,
        xmp_metadata=_parse_xmp_metadata(meta.xmp_metadata),
        is_encrypted=reader.is_encrypted,
    )


class ZIPMetadata(ValidatorModel,Base):
    """Stores file metadata for individual zip compressed files.
    See https://docs.python.org/3/library/zipfile.html#zipinfo-objects for more details.

    Args:
        Base (_type_): _description_
    """
    model_config = ConfigDict(extra='ignore')  

    filename: str
    last_modified: List[str]
    compress_type: str
    comment: str
    extra: Optional[str]
    version: Optional[int]
    header_offset: Optional[int]
    crc: Optional[str]
    compress_size: Optional[int]
    file_size: Optional[int]
    
def run_7z_list(archive_path: str) -> Union[str, bool]:
    """
    Executes the 7z list command to get detailed information about the contents of an archive.

    Args:
        archive_path: The filesystem path to the archive file.

    Returns:
        The stdout from the 7z list command as a string, or False if the command fails.
    """
    command = [WIN_7Z_PATH, 'l', '-slt', '-ssp', archive_path]
    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if result.returncode != 0:
        logging.error(f"Error running 7z list command: {result.stderr}")
    if result.returncode != 0:
        return False  # Indicate failure without raising an exception
    
    return result.stdout

def parse_7z_output(output: Union[str, bool]) -> Union[list[dict[str, str]], bool]:
    """Parses the detailed output from the 7z list command into structured data.

    This function handles the parsing of the 7z command output, transforming it into
    a list of dictionaries where each dictionary represents the metadata for a single file
    in the archive. It safely skips over lines that do not conform to the expected
    "key = value" format, such as warnings or malformed output.

    Args:
        output: The stdout string from the 7z list command or False if the command failed.

    Returns:
        A list of dictionaries, each representing file metadata from the archive, or False if input is False.
    """
    if output is False:
        # Early return if the 7z list command failed, indicating no data to parse.
        return False
    
    lines = output.replace('\r\n', '\n').split('\n')
    files_data = []  # Accumulator for all file metadata dictionaries.
    current_file_data = {}  # Temporary storage for the current file's metadata.
    capture_data = False  # Flag indicating whether we are currently parsing file metadata.
    
    for line in lines:
        line = line.strip()
        if line == '----------':
            # Toggle capturing on encountering the delimiter, indicating start or end of file metadata.
            if capture_data and current_file_data:
                # End of a file's metadata; add it to the list and prepare for the next file.
                files_data.append(current_file_data)
                current_file_data = {}
            capture_data = not capture_data
        elif capture_data and line:
            # Only attempt to parse lines that contain an '=' and are within a metadata section.
            if '=' in line:
                # Split the line at the first '=' and provide a default empty value if none exists.
                parts = line.split('=', 1)
                key = parts[0].strip().lower()
                value = parts[1].strip() if len(parts) > 1 else ""
                current_file_data[key] = value
            # Lines without an '=' are ignored, preventing errors from unexpected format.
        elif capture_data and not line and current_file_data:
            # An empty line within a metadata section signifies the end of the current file's metadata.
            files_data.append(current_file_data)
            current_file_data = {}

    # Add the last file's data if it exists and wasn't added in the loop.
    if current_file_data:
        files_data.append(current_file_data)
    
    return files_data

def to_int(value, default=0):
    """
    Safely converts a value to an integer, returning `default` if the conversion fails.

    Args:
        value: The value to convert to an integer.
        default (int, optional): The default value to return if conversion fails. Defaults to 0.

    Returns:
        int: The converted integer value or `default` if the conversion fails.
    """
    if not value:
        return default
    try:
        return int(value) if value != "" else default
    except ValueError:
        return default


def list_archive_contents(archive_path: str) -> dict[str, str]:
    """Lists the contents of an archive file and returns the metadata as a JSON string or False if failed.

    Args:
        archive_path: The filesystem path to the archive file.

    Returns:
        A dict representing the metadata of files within the archive, or False if the 7z command fails.
    """
    output = run_7z_list(archive_path)
    if output is False:
        return False  # Indicate failure to list archive contents
    
    files_data = parse_7z_output(output)
    if files_data is False:
        return {}
    
    return files_data

def get_zip_metadata(file: Path) -> list[ZIPMetadata]:
    archive_content = list_archive_contents(file)
    list_zip_meta =  []
    for data in archive_content:
        list_zip_meta.append(
            ZIPMetadata(
                filename=data.get('path', ''),
                last_modified=data.get('modified',''),
                compress_type=data.get('method', 'Deflate:Fastest'),
                comment=data.get('comment', '').encode("latin1"),
                extra=data.get('characteristics', '').encode("latin1"),
                version=to_int(data.get('version')),
                header_offset=to_int(data.get('offset')),
                crc=to_int(data.get('crc', '0'), 16),
                compress_size=to_int(data.get('packed size')),
                file_size=to_int(data.get('size')),
            )
        )
    return list_zip_meta

class RARMetadata(Base):
    """Stores file metadata for individual rar compressed files.
    See https://rarfile.readthedocs.io/api.html#rarinfo-class for more details.
    """

    filename: str
    last_modified: Optional[datetime]
    compress_type: int
    comment: Optional[HexBytes]
    version: Optional[int]
    crc: Optional[int]
    compress_size: int
    file_size: int
    compress_type: Optional[str]
    host_os: Optional[str]
    mode: Optional[str]
    creation_time: Optional[datetime]
    last_access_time: Optional[datetime]
    archival_time: Optional[datetime]
    blake2sp_hash: Optional[str]
    volume: Optional[int]
    is_dir: bool
    file_redir: Optional[str]
    is_symlink: bool
    is_file: bool
    needs_password: bool


def get_rar_metadata(file: Path) -> List[RARMetadata]:
    rf = rarfile.RarFile(file)
    return [
        RARMetadata(
            filename=f.filename,
            last_modified=f.mtime,
            comment=f.comment,
            file_size=f.file_size,
            compress_size=f.compress_size,
            compress_type=str(f.compress_type),
            version=f.extract_version,
            host_os=str(f.host_os),
            mode=str(f.mode),
            creation_time=f.ctime,
            last_access_time=f.atime,
            archival_time=f.arctime,
            crc=f.CRC,
            blake2sp_hash=f.blake2sp_hash,
            volume=f.volume,
            file_redir=str(f.file_redir),
            is_dir=f.is_dir(),
            is_symlink=f.is_symlink(),
            is_file=f.is_file(),
            needs_password=f.needs_password(),
        )
        for f in rf.infolist()
    ]
